{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRTMPFP2vGIU"
   },
   "source": [
    "## Actualizando la información de los LLMs\n",
    "\n",
    "Una gran limitación de los  LLMS es que almacenan toda la información en sus parámetros que son ajustados en el momento de entrenamiento. Dentro de la memoria de ChatGPT hay almacenada una gran cantidad de datos de la realidad que:\n",
    "* Se repitieron en los textos de entrenamiento de la etapa no supervisada (predicción del texto que sigue)\n",
    "\n",
    "* Fueron parte de las respuestas escritas por humanos en la etapa supervisada\n",
    "\n",
    "* Fueron etiquetadas por un humano como correctas (:thumbsup:) en la etapa de RLHF\n",
    "\n",
    "Miren el siguiente ejemplo:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u28gLBDJwVzt"
   },
   "source": [
    "<img src=\"https://i.ibb.co/wp3vrv5/Screen-Shot-2023-09-17-at-17-03-57.png\" width=\"500px\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_LP9EDnxEQl"
   },
   "source": [
    "Pero ¿Qué pasa si necesitamos datos recientes?\n",
    "\n",
    "<img src=\"https://i.ibb.co/rMj3ScT/Whats-App-Image-2023-09-13-at-13-36-03-1.jpg\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TD1AN2yyP3R"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Tiene formato de código\n",
    "```\n",
    "\n",
    "Desde la aparición de los LLMs cobraron gran importancia las herramientas capaces de proveer información textual en tiempo real, como las bases de datos vectoriales (que vamos a ver más adelante en el curso) y los motores de búsqueda consumidos a través de APIs como Bing y <a href=\"https://serpapi.com/\">SERP API</a>, que utiliza web scraping de los resultados orgánicos de Google.\n",
    "\n",
    "Existen aplicaciones para usuarios finales que ya permiten combinar la capacidad de razonamiento de los LLMs con estos grandes índices de información actualizada como https://www.bing.com/chat y https://www.perplexity.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bb606YkDzneD"
   },
   "source": [
    "<img src=\"https://i.ibb.co/VQ27VQZ/Whats-App-Image-2023-09-13-at-13-36-03.jpg\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4k8usXD0IN5"
   },
   "source": [
    "## Ejercicio\n",
    "\n",
    "Les proponemos utilizar las APIs de SERP y OpenAI para responder preguntas de actualidad, como por ejemplo ¿Quién ganó las elecciones de Argentina 2023? Exploremos un sistema capaz de responder a cualquier pregunta de actualidad.\n",
    "\n",
    "Para poder resolver este ejercicio vamos a utilizar dos sistemas externos: SERP API y OpenAI. Ambos tienen consumos de prueba que se pueden utilizar para explorar, pero cuando integramos este tipo de servicios a nuestras tenemos que considerar:\n",
    "\n",
    "1) Los costos: https://serpapi.com/pricing // https://openai.com/pricing\n",
    "\n",
    "2) Seguridad: ¡Cuidado! Las claves privadas son lo único que se necesita para acceder con una identidad. En los sistemas productivos las claves se encuentran en un archivo de configuración (.env) y JAMÁS se deben subir a los repositorios de código como git. Normalmente se excluye al .env del repositorio utilizando el archivo .gitignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y1I3fR-lWxtl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting openai\n",
      "  Downloading openai-1.102.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting dotenv\n",
      "  Using cached dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting requests (from google-search-results)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kobou\\onedrive\\documentos\\personal\\utn-agro-ai\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->google-search-results)\n",
      "  Using cached charset_normalizer-3.4.3-cp312-cp312-win_amd64.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->google-search-results)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Downloading openai-1.102.0-py3-none-any.whl (812 kB)\n",
      "   ---------------------------------------- 0.0/812.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 812.0/812.0 kB 8.7 MB/s  0:00:00\n",
      "Using cached anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jiter-0.10.0-cp312-cp312-win_amd64.whl (206 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (pyproject.toml): started\n",
      "  Building wheel for google-search-results (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32173 sha256=116623f17e797e01601c8419fd1f0dca404478fe3a8cfaa7a7b13b91aa62468f\n",
      "  Stored in directory: c:\\users\\kobou\\appdata\\local\\pip\\cache\\wheels\\0c\\47\\f5\\89b7e770ab2996baf8c910e7353d6391e373075a0ac213519e\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, sniffio, python-dotenv, jiter, idna, h11, distro, charset_normalizer, certifi, annotated-types, typing-inspection, requests, pydantic-core, httpcore, dotenv, anyio, pydantic, httpx, google-search-results, openai\n",
      "\n",
      "   ----------------------------------------  0/22 [urllib3]\n",
      "   ----------------------------------------  0/22 [urllib3]\n",
      "   ----------------------------------------  0/22 [urllib3]\n",
      "   ----------------------------------------  0/22 [urllib3]\n",
      "   ----------------------------------------  0/22 [urllib3]\n",
      "   ----------------------------------------  0/22 [urllib3]\n",
      "   --- ------------------------------------  2/22 [tqdm]\n",
      "   --- ------------------------------------  2/22 [tqdm]\n",
      "   --- ------------------------------------  2/22 [tqdm]\n",
      "   --- ------------------------------------  2/22 [tqdm]\n",
      "   --- ------------------------------------  2/22 [tqdm]\n",
      "   --- ------------------------------------  2/22 [tqdm]\n",
      "   --- ------------------------------------  2/22 [tqdm]\n",
      "   ----- ----------------------------------  3/22 [sniffio]\n",
      "   ------- --------------------------------  4/22 [python-dotenv]\n",
      "   ------- --------------------------------  4/22 [python-dotenv]\n",
      "   ---------- -----------------------------  6/22 [idna]\n",
      "   ---------- -----------------------------  6/22 [idna]\n",
      "   ------------ ---------------------------  7/22 [h11]\n",
      "   ------------ ---------------------------  7/22 [h11]\n",
      "   -------------- -------------------------  8/22 [distro]\n",
      "   -------------- -------------------------  8/22 [distro]\n",
      "   ---------------- -----------------------  9/22 [charset_normalizer]\n",
      "   ---------------- -----------------------  9/22 [charset_normalizer]\n",
      "   ---------------- -----------------------  9/22 [charset_normalizer]\n",
      "   -------------------- ------------------- 11/22 [annotated-types]\n",
      "   ----------------------- ---------------- 13/22 [requests]\n",
      "   ----------------------- ---------------- 13/22 [requests]\n",
      "   ----------------------- ---------------- 13/22 [requests]\n",
      "   --------------------------- ------------ 15/22 [httpcore]\n",
      "   --------------------------- ------------ 15/22 [httpcore]\n",
      "   --------------------------- ------------ 15/22 [httpcore]\n",
      "   --------------------------- ------------ 15/22 [httpcore]\n",
      "   --------------------------- ------------ 15/22 [httpcore]\n",
      "   ------------------------------ --------- 17/22 [anyio]\n",
      "   ------------------------------ --------- 17/22 [anyio]\n",
      "   ------------------------------ --------- 17/22 [anyio]\n",
      "   ------------------------------ --------- 17/22 [anyio]\n",
      "   ------------------------------ --------- 17/22 [anyio]\n",
      "   ------------------------------ --------- 17/22 [anyio]\n",
      "   ------------------------------ --------- 17/22 [anyio]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [pydantic]\n",
      "   ---------------------------------- ----- 19/22 [httpx]\n",
      "   ---------------------------------- ----- 19/22 [httpx]\n",
      "   ---------------------------------- ----- 19/22 [httpx]\n",
      "   ---------------------------------- ----- 19/22 [httpx]\n",
      "   ---------------------------------- ----- 19/22 [httpx]\n",
      "   ---------------------------------- ----- 19/22 [httpx]\n",
      "   ------------------------------------ --- 20/22 [google-search-results]\n",
      "   ------------------------------------ --- 20/22 [google-search-results]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   -------------------------------------- - 21/22 [openai]\n",
      "   ---------------------------------------- 22/22 [openai]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.10.0 certifi-2025.8.3 charset_normalizer-3.4.3 distro-1.9.0 dotenv-0.9.9 google-search-results-2.4.2 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.10.0 openai-1.102.0 pydantic-2.11.7 pydantic-core-2.33.2 python-dotenv-1.1.1 requests-2.32.5 sniffio-1.3.1 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspection-0.4.1 urllib3-2.5.0\n"
     ]
    }
   ],
   "source": [
    "# Instalamos las librerías\n",
    "!pip install google-search-results openai dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J89ahGqro-Km"
   },
   "outputs": [],
   "source": [
    "# Paso 1. Registrarse a SERP API y Open AI para obetener las siguientes claves\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SERP_API_KEY = os.getenv(\"SERP_API\")\n",
    "OPENAI_API_KEY= os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hvmGwWXXwRce"
   },
   "outputs": [],
   "source": [
    "# Paso 2.Siguiendo la documentación de SERP API escriban una función para buscar en SERP. Por default vamos a querer 5 resultados\n",
    "\n",
    "from itertools import islice\n",
    "from dataclasses import dataclass\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SnippetCitation:\n",
    "    Url: str\n",
    "    Title: str\n",
    "    Snippet: str\n",
    "\n",
    "\n",
    "\n",
    "def serp_results(query: str, num=5, api_key=SERP_API_KEY):\n",
    "  params = {\n",
    "      \"q\": query,\n",
    "      \"api_key\": api_key,\n",
    "      \"num\": num,\n",
    "      \"engine\": \"google\",\n",
    "      \"hl\": \"es\"\n",
    "  }\n",
    "  search = GoogleSearch(params)\n",
    "  res = search.get_dict()\n",
    "  organic_results = res[\"organic_results\"]\n",
    "  return_results = []\n",
    "  for result in organic_results:\n",
    "      try:\n",
    "        return_results.append(SnippetCitation(Url=result[\"link\"], Title=result[\"title\"], Snippet=result[\"snippet\"]))\n",
    "      except Exception as e:\n",
    "          print(e)\n",
    "  return return_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Rkwwb-E7asDf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SnippetCitation(Url='https://es.wikipedia.org/wiki/Elecciones_presidenciales_de_Croacia_de_2024-25', Title='Elecciones presidenciales de Croacia de 2024-25', Snippet='Las elecciones presidenciales de Croacia se realizaron el 29 de diciembre de 2024. Como ningún candidato obtuvo la mayoría de votos se procedió con la ...'),\n",
       " SnippetCitation(Url='https://es.euronews.com/my-europe/2025/01/12/milanovic-gana-las-presidenciales-de-croacia-por-un-amplio-margen-segun-los-sondeos-a-pie-', Title='Milanovic gana las elecciones presidenciales de Croacia ...', Snippet='Los resultados oficiales muestran que el actual presidente, Zoran Milanovic, logra la reelección con una abrumadora mayoría, al cosechar más ...'),\n",
       " SnippetCitation(Url='https://translate.google.com/translate?u=https://en.wikipedia.org/wiki/2024%25E2%2580%259325_Croatian_presidential_election&hl=es&sl=en&tl=es&client=srp', Title='Elecciones presidenciales croatas 2024-25', Snippet='Las elecciones presidenciales en Croacia se celebraron el 29 de diciembre de 2024, con una segunda vuelta el 12 de enero de 2025, tras la mayoría de votos ...'),\n",
       " SnippetCitation(Url='https://www.dw.com/es/zoran-milanovic-es-reelegido-presidente-de-croacia-con-victoria-aplastante/a-71279743', Title='Zoran Milanovic es reelegido presidente de Croacia', Snippet='El nacionalista, euroescéptico y prorruso candidato ganó 74% de los votos. Su rival conservador Dragan Primorac obtuvo 26%.'),\n",
       " SnippetCitation(Url='https://translate.google.com/translate?u=https://www.aljazeera.com/news/2025/1/12/incumbent-expected-to-win-as-croatians-vote-in-presidential-run-off&hl=es&sl=en&tl=es&client=srp', Title='El presidente croata Milanovic fue reelegido con una ...', Snippet='Zoran Milanovic, un crítico de la OTAN, gana cómodamente la reelección, obteniendo el 74 por ciento de los votos.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serp_res = serp_results(\"¿Quién gano las elecciones presidenciales en Croacia en 2025?\")\n",
    "serp_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9gPBfR7b9xh"
   },
   "source": [
    "# Paso 3\n",
    "\n",
    "Utilizar GPT para responder a la pregunta.\n",
    "\n",
    "Siguiendo la documentación de la API de OpenAI, escriban una función para generar texto simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZA6RjUKUa03L"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iwQCl9r6NpQL"
   },
   "outputs": [],
   "source": [
    "  # Uses OPENAI_API_KEY environment variable\n",
    "\n",
    "def chat_complete(\n",
    "    syst: str | None,\n",
    "    user: list[str] = [],\n",
    "    assistant: list[str] = [],\n",
    "    max_tokens: int = 1024,\n",
    "    temperature: float = 0,\n",
    "    model: str = \"gpt-3.5-turbo\" , # Elijan el modelo por default. ¿Cuánto cuesta por cada 1000 tokens de output? ¿Y de input? https://platform.openai.com/docs/models\n",
    ") -> str:\n",
    "    # Initialize the OpenAI client\n",
    "    messages: list[dict[str, str]] = []\n",
    "\n",
    "    if syst is not None:\n",
    "        messages.append({\"role\": \"system\", \"content\": syst})\n",
    "\n",
    "    for i in range(len(user)):\n",
    "        messages.append({\"role\": \"user\", \"content\": user[i]})\n",
    "        if len(assistant) > i:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant[i]})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oTJJ4KlqeYbK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_complete(\"Hola\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbFIFJICfPsM"
   },
   "source": [
    "# Paso 4\n",
    "\n",
    "Pensemos un buen prompt que pueda tomar como contexto la información obtenida de SERP API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjLCl-LbedWk"
   },
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = \"\"\"Tu objetivo es responder a la pregunta [PREGUNTA] utilizando este contexto [CONTEXTO].\n",
    "\n",
    "Si la respuesta no se encuentra en el contexto, responde que no sabes. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vp7OAn8NfvMV"
   },
   "outputs": [],
   "source": [
    "pregunta = \"¿Quién gano las elecciones presidenciales en Croacia en 2025?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbImV4Qgf2uE"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = SYSTEM_PROMPT.replace(\"[PREGUNTA]\", pregunta)\n",
    "SYSTEM_PROMPT = SYSTEM_PROMPT.replace(\"[CONTEXTO]\", str(serp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uy0UA0DfgI_x"
   },
   "outputs": [],
   "source": [
    "chat_complete(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scDeCPb0TYFQ"
   },
   "source": [
    "# Paso 6\n",
    "\n",
    "Darle una forma estructurada a las respuestas.\n",
    "\n",
    "Ahora vamos a adaptar la función chat_complete para recibir como parámetro el schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDPMA07dSZNV"
   },
   "outputs": [],
   "source": [
    "def limpiar_markdown(content: str):\n",
    "    # Remove markdown code block markers\n",
    "    content = content.strip()\n",
    "    if content.startswith('```json'):\n",
    "        content = content[7:]  # Remove ```json\n",
    "    elif content.startswith('```'):\n",
    "        content = content[3:]   # Remove ```\n",
    "    if content.endswith('```'):\n",
    "        content = content[:-3]  # Remove closing ```\n",
    "    content = content.strip()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGProC82RiGG"
   },
   "outputs": [],
   "source": [
    "def chat_complete(\n",
    "    syst: str | None,\n",
    "    user: list[str] = [],\n",
    "    assistant: list[str] = [],\n",
    "    max_tokens: int = 1024,\n",
    "    temperature: float = 0,\n",
    "    model: str = \"gpt-4o\",\n",
    "    schema: dict | None = None\n",
    ") -> str:\n",
    "    # Initialize the OpenAI client\n",
    "    messages: list[dict[str, str]] = []\n",
    "\n",
    "    if syst is not None:\n",
    "        messages.append({\"role\": \"system\", \"content\": syst})\n",
    "\n",
    "    for i in range(len(user)):\n",
    "        messages.append({\"role\": \"user\", \"content\": user[i]})\n",
    "        if len(assistant) > i:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant[i]})\n",
    "\n",
    "    # Build the request parameters\n",
    "    request_params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "    if schema is not None:\n",
    "        request_params[\"response_format\"] = {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"structured_response\",\n",
    "                \"schema\": schema,\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }\n",
    "    response = client.chat.completions.create(**request_params)\n",
    "    content = response.choices[0].message.content\n",
    "    content = limpiar_markdown(content)\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Iu23GuzXOIs"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Tu objetivo es responder a la pregunta [PREGUNTA] utilizando este contexto [CONTEXTO].\n",
    "\n",
    "El formato de salida debe ser un array de json con todas las entidades mencionadas. Cada una representada como un único string del título \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRqCf2OOXZh2"
   },
   "outputs": [],
   "source": [
    "pregunta = \"Recomendaciones de series 2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Iw1hcp8XooM"
   },
   "outputs": [],
   "source": [
    "serp_res = serp_results(pregunta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmMaZnWhXwx2"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = SYSTEM_PROMPT.replace(\"[PREGUNTA]\", pregunta)\n",
    "SYSTEM_PROMPT = SYSTEM_PROMPT.replace(\"[CONTEXTO]\", str(serp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5pBjffiQ5nk"
   },
   "outputs": [],
   "source": [
    "# Ahora representemos en un diccionario un esquema con un array de string\n",
    "RECOMMENDATIONS_SCHEMA = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"STRING\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZ4qjvnyX2Gf"
   },
   "outputs": [],
   "source": [
    "json_list = chat_complete(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cxb4C9QXX3_W"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(json_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2w6JY4wZBxp"
   },
   "source": [
    "# Paso 7. Bonus\n",
    "\n",
    "Como extra, hagamos una función que nos indique dónde ver cada una de las series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoiUSJzFZNHs"
   },
   "outputs": [],
   "source": [
    "shows = json.loads(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xN8xEtzWZO3e"
   },
   "outputs": [],
   "source": [
    "def get_streaming_service(show:str):\n",
    "  SYSTEM_PROMPT = \"\"\"Tu objetivo es responder a la pregunta [PREGUNTA] utilizando este contexto [CONTEXTO].\"\"\"\n",
    "  pregunta = f\"¿Dónde puedo ver la serie {show}?\"\n",
    "  contexto = serp_results(pregunta)\n",
    "  SYSTEM_PROMPT = SYSTEM_PROMPT.replace(\"[PREGUNTA]\", pregunta)\n",
    "  SYSTEM_PROMPT = SYSTEM_PROMPT.replace(\"[CONTEXTO]\", str(contexto))\n",
    "  streaming = chat_complete(SYSTEM_PROMPT)\n",
    "  print(show,\"-->\" ,streaming)\n",
    "  return (show, streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qy8HyKlDaM8D"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "# Hagamos sólo 5 para ahorrar costos\n",
    "for show in shows[:5]:\n",
    "  results.append(get_streaming_service(show))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1VC7ySx7-CnjyWDQZjzm0Do1gGRanlGBO",
     "timestamp": 1754675092336
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
