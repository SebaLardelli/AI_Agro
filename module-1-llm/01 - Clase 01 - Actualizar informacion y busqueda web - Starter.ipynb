{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRTMPFP2vGIU"
   },
   "source": [
    "## Actualizando la información de los LLMs\n",
    "\n",
    "Una gran limitación de los  LLMS es que almacenan toda la información en sus parámetros que son ajustados en el momento de entrenamiento. Dentro de la memoria de ChatGPT hay almacenada una gran cantidad de datos de la realidad que:\n",
    "* Se repitieron en los textos de entrenamiento de la etapa no supervisada (predicción del texto que sigue)\n",
    "\n",
    "* Fueron parte de las respuestas escritas por humanos en la etapa supervisada\n",
    "\n",
    "* Fueron etiquetadas por un humano como correctas (:thumbsup:) en la etapa de RLHF\n",
    "\n",
    "Miren el siguiente ejemplo:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u28gLBDJwVzt"
   },
   "source": [
    "<img src=\"https://i.ibb.co/wp3vrv5/Screen-Shot-2023-09-17-at-17-03-57.png\" width=\"500px\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_LP9EDnxEQl"
   },
   "source": [
    "Pero ¿Qué pasa si necesitamos datos recientes?\n",
    "\n",
    "<img src=\"https://i.ibb.co/rMj3ScT/Whats-App-Image-2023-09-13-at-13-36-03-1.jpg\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TD1AN2yyP3R"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Tiene formato de código\n",
    "```\n",
    "\n",
    "Desde la aparición de los LLMs cobraron gran importancia las herramientas capaces de proveer información textual en tiempo real, como las bases de datos vectoriales (que vamos a ver más adelante en el curso) y los motores de búsqueda consumidos a través de APIs como Bing y <a href=\"https://serpapi.com/\">SERP API</a>, que utiliza web scraping de los resultados orgánicos de Google.\n",
    "\n",
    "Existen aplicaciones para usuarios finales que ya permiten combinar la capacidad de razonamiento de los LLMs con estos grandes índices de información actualizada como https://www.bing.com/chat y https://www.perplexity.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bb606YkDzneD"
   },
   "source": [
    "<img src=\"https://i.ibb.co/VQ27VQZ/Whats-App-Image-2023-09-13-at-13-36-03.jpg\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4k8usXD0IN5"
   },
   "source": [
    "## Ejercicio\n",
    "\n",
    "Les proponemos utilizar las APIs de SERP y OpenAI para responder preguntas de actualidad, como por ejemplo ¿Quién ganó las elecciones de Argentina 2023? Exploremos un sistema capaz de responder a cualquier pregunta de actualidad.\n",
    "\n",
    "Para poder resolver este ejercicio vamos a utilizar dos sistemas externos: SERP API y OpenAI. Ambos tienen consumos de prueba que se pueden utilizar para explorar, pero cuando integramos este tipo de servicios a nuestras tenemos que considerar:\n",
    "\n",
    "1) Los costos: https://serpapi.com/pricing // https://openai.com/pricing\n",
    "\n",
    "2) Seguridad: ¡Cuidado! Las claves privadas son lo único que se necesita para acceder con una identidad. En los sistemas productivos las claves se encuentran en un archivo de configuración (.env) y JAMÁS se deben subir a los repositorios de código como git. Normalmente se excluye al .env del repositorio utilizando el archivo .gitignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12942,
     "status": "ok",
     "timestamp": 1755904388190,
     "user": {
      "displayName": "Matias Barreto (devenirfantasma)",
      "userId": "07072764524169679349"
     },
     "user_tz": 180
    },
    "id": "y1I3fR-lWxtl",
    "outputId": "0b5ffcac-513c-4a3c-af0b-8c130c4e83cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.100.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from google-search-results) (2.32.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->google-search-results) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->google-search-results) (2.5.0)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=66158e03f34376fa8dc95ef858e189865cd881813f4e7ea471306d68954a7126\n",
      "  Stored in directory: /root/.cache/pip/wheels/0c/47/f5/89b7e770ab2996baf8c910e7353d6391e373075a0ac213519e\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n"
     ]
    }
   ],
   "source": [
    "# Instalamos las librerías\n",
    "!pip install google-search-results openai dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9172,
     "status": "ok",
     "timestamp": 1755905218070,
     "user": {
      "displayName": "Matias Barreto (devenirfantasma)",
      "userId": "07072764524169679349"
     },
     "user_tz": 180
    },
    "id": "J89ahGqro-Km"
   },
   "outputs": [],
   "source": [
    "# Paso 1. Registrarse a SERP API y Open AI para obetener las siguientes claves\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "SERP_API_KEY = os.getenv(\"SERP_API_KEY\")\n",
    "#OPENAI_ORGANIZATION = userdata.get(\"OPENAI_ORGANIZATION\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "error",
     "timestamp": 1755904422447,
     "user": {
      "displayName": "Matias Barreto (devenirfantasma)",
      "userId": "07072764524169679349"
     },
     "user_tz": 180
    },
    "id": "hvmGwWXXwRce",
    "outputId": "9d210ffe-71aa-48e6-8321-f2f91534c9c9"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (ipython-input-4280338802.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4280338802.py\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    params = ? # Busquen los parámetros en la documentación de SERP\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Paso 2.Siguiendo la documentación de SERP API escriban una función para buscar en SERP. Por default vamos a querer 5 resultados\n",
    "\n",
    "from itertools import islice\n",
    "from dataclasses import dataclass\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SnippetCitation:\n",
    "    Url: str\n",
    "    Title: str\n",
    "    Snippet: str\n",
    "\n",
    "\n",
    "\n",
    "def serp_results(query: str, num=5, api_key=SERP_API_KEY):\n",
    "  params = ? # Busquen los parámetros en la documentación de SERP\n",
    "  search = GoogleSearch(params)\n",
    "  res = search.get_dict()\n",
    "  organic_results = res[\"organic_results\"]\n",
    "  return_results = []\n",
    "  for result in organic_results:\n",
    "      try:\n",
    "        return_results.append(SnippetCitation(Url=result[\"link\"], Title=result[\"title\"], Snippet=result[\"snippet\"]))\n",
    "      except Exception as e:\n",
    "          print(e)\n",
    "  return return_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1755904942151,
     "user": {
      "displayName": "Matias Barreto (devenirfantasma)",
      "userId": "07072764524169679349"
     },
     "user_tz": 180
    },
    "id": "Rkwwb-E7asDf",
    "outputId": "8b935429-f786-4205-a6e9-c466015925a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serp_res = serp_results(\"¿Quién gano las elecciones presidenciales en Croacia en 2025?\")\n",
    "serp_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9gPBfR7b9xh"
   },
   "source": [
    "# Paso 3\n",
    "\n",
    "Utilizar GPT para responder a la pregunta.\n",
    "\n",
    "Siguiendo la documentación de la API de OpenAI, escriban una función para generar texto simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZA6RjUKUa03L"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = # ¿Cómo crear el cliente utilizando la API key?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwQCl9r6NpQL"
   },
   "outputs": [],
   "source": [
    "  # Uses OPENAI_API_KEY environment variable\n",
    "\n",
    "def chat_complete(\n",
    "    syst: str | None,\n",
    "    user: list[str] = [],\n",
    "    assistant: list[str] = [],\n",
    "    max_tokens: int = 1024,\n",
    "    temperature: float = 0,\n",
    "    model: str = ?, # Elijan el modelo por default. ¿Cuánto cuesta por cada 1000 tokens de output? ¿Y de input? https://platform.openai.com/docs/models\n",
    ") -> str:\n",
    "    # Initialize the OpenAI client\n",
    "    messages: list[dict[str, str]] = []\n",
    "\n",
    "    if syst is not None:\n",
    "        messages.append({\"role\": \"system\", \"content\": syst})\n",
    "\n",
    "    for i in range(len(user)):\n",
    "        messages.append({\"role\": \"user\", \"content\": user[i]})\n",
    "        if len(assistant) > i:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant[i]})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTJJ4KlqeYbK"
   },
   "outputs": [],
   "source": [
    "chat_complete(\"Hola\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbFIFJICfPsM"
   },
   "source": [
    "# Paso 4\n",
    "\n",
    "Pensemos un buen prompt que pueda tomar como contexto la información obtenida de SERP API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjLCl-LbedWk"
   },
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = \"\"\"Tu objetivo es responder a la pregunta [PREGUNTA] utilizando este contexto [CONTEXTO].\n",
    "\n",
    "Si la respuesta no se encuentra en el contexto, responde que no sabes. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vp7OAn8NfvMV"
   },
   "outputs": [],
   "source": [
    "pregunta = \"¿Quién gano las elecciones presidenciales en Croacia en 2025?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbImV4Qgf2uE"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = SYSTEM_PROMPT.replace(\"[PREGUNTA]\", pregunta)\n",
    "SYSTEM_PROMPT = SYSTEM_PROMPT.replace(\"[CONTEXTO]\", str(serp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uy0UA0DfgI_x"
   },
   "outputs": [],
   "source": [
    "chat_complete(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scDeCPb0TYFQ"
   },
   "source": [
    "# Paso 6\n",
    "\n",
    "Darle una forma estructurada a las respuestas.\n",
    "\n",
    "Ahora vamos a adaptar la función chat_complete para recibir como parámetro el schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDPMA07dSZNV"
   },
   "outputs": [],
   "source": [
    "def limpiar_markdown(content: str):\n",
    "    # Remove markdown code block markers\n",
    "    content = content.strip()\n",
    "    if content.startswith('```json'):\n",
    "        content = content[7:]  # Remove ```json\n",
    "    elif content.startswith('```'):\n",
    "        content = content[3:]   # Remove ```\n",
    "    if content.endswith('```'):\n",
    "        content = content[:-3]  # Remove closing ```\n",
    "    content = content.strip()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGProC82RiGG"
   },
   "outputs": [],
   "source": [
    "def chat_complete(\n",
    "    syst: str | None,\n",
    "    user: list[str] = [],\n",
    "    assistant: list[str] = [],\n",
    "    max_tokens: int = 1024,\n",
    "    temperature: float = 0,\n",
    "    model: str = \"gpt-4o\",\n",
    "    schema: dict | None = None\n",
    ") -> str:\n",
    "    # Initialize the OpenAI client\n",
    "    messages: list[dict[str, str]] = []\n",
    "\n",
    "    if syst is not None:\n",
    "        messages.append({\"role\": \"system\", \"content\": syst})\n",
    "\n",
    "    for i in range(len(user)):\n",
    "        messages.append({\"role\": \"user\", \"content\": user[i]})\n",
    "        if len(assistant) > i:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant[i]})\n",
    "\n",
    "    # Build the request parameters\n",
    "    request_params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "    if schema is not None:\n",
    "        request_params[\"response_format\"] = {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"structured_response\",\n",
    "                \"schema\": schema,\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }\n",
    "    response = client.chat.completions.create(**request_params)\n",
    "    content = response.choices[0].message.content\n",
    "    content = limpiar_markdown(content)\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Iu23GuzXOIs"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Tu objetivo es responder a la pregunta [PREGUNTA] utilizando este contexto [CONTEXTO].\n",
    "\n",
    "El formato de salida debe ser un array de json con todas las entidades mencionadas. Cada una representada como un único string del título \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRqCf2OOXZh2"
   },
   "outputs": [],
   "source": [
    "pregunta = \"Recomendaciones de series 2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Iw1hcp8XooM"
   },
   "outputs": [],
   "source": [
    "serp_res = serp_results(pregunta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmMaZnWhXwx2"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = SYSTEM_PROMPT.replace(\"[PREGUNTA]\", pregunta)\n",
    "SYSTEM_PROMPT = SYSTEM_PROMPT.replace(\"[CONTEXTO]\", str(serp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5pBjffiQ5nk"
   },
   "outputs": [],
   "source": [
    "# Ahora representemos en un diccionario un esquema con un array de string\n",
    "RECOMMENDATIONS_SCHEMA = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"STRING\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZ4qjvnyX2Gf"
   },
   "outputs": [],
   "source": [
    "json_list = chat_complete(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cxb4C9QXX3_W"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(json_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2w6JY4wZBxp"
   },
   "source": [
    "# Paso 7. Bonus\n",
    "\n",
    "Como extra, hagamos una función que nos indique dónde ver cada una de las series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoiUSJzFZNHs"
   },
   "outputs": [],
   "source": [
    "shows = json.loads(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xN8xEtzWZO3e"
   },
   "outputs": [],
   "source": [
    "def get_streaming_service(show:str):\n",
    "  SYSTEM_PROMPT = \"\"\"Tu objetivo es responder a la pregunta [PREGUNTA] utilizando este contexto [CONTEXTO].\"\"\"\n",
    "  pregunta = f\"¿Dónde puedo ver la serie {show}?\"\n",
    "  contexto = serp_results(pregunta)\n",
    "  SYSTEM_PROMPT = SYSTEM_PROMPT.replace(\"[PREGUNTA]\", pregunta)\n",
    "  SYSTEM_PROMPT = SYSTEM_PROMPT.replace(\"[CONTEXTO]\", str(contexto))\n",
    "  streaming = chat_complete(SYSTEM_PROMPT)\n",
    "  print(show,\"-->\" ,streaming)\n",
    "  return (show, streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qy8HyKlDaM8D"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "# Hagamos sólo 5 para ahorrar costos\n",
    "for show in shows[:5]:\n",
    "  results.append(get_streaming_service(show))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1WZ5NZn5YPSSoQ0MhILiBx6NoEzDIMGoO",
     "timestamp": 1755898317819
    },
    {
     "file_id": "1VC7ySx7-CnjyWDQZjzm0Do1gGRanlGBO",
     "timestamp": 1754675092336
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
